{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run those lines if you haven't installed gym yet.\n",
    "# !pip install --upgrade gym > gym.log\n",
    "\n",
    "# # the lines below is only needed if you run on an everware server\n",
    "# !apt-get -qq update \n",
    "# !apt-get install -y xvfb > xvfb.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "# don't run if on laptop\n",
    "#import os\n",
    "#if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "#    !bash ../xvfb start\n",
    "#    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice: \"deep\" crossentropy method with neural nets\n",
    "This notebook will teach you to solve reinforcement learning problems with crossentropy method with neural network policy.\n",
    "\n",
    "![img](https://casd35.wikispaces.com/file/view/digging_deeper_final.jpg/359658499/503x260/digging_deeper_final.jpg)\n",
    "\n",
    "In this section we will train a neural network policy for continuous state space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-25 14:25:26,863] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11d592898>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c7b7438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-25f20cb047e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "s = env.state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sessions\n",
    "\n",
    "In the next section we can use agent's predict_proba method to sample actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0] \n",
    "        \n",
    "        a = <sample action with such probabilities>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,a,r = generate_session()\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) in [float,np.float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see the initial reward distribution\n",
    "sample_rewards = [generate_session()[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards,bins=20);\n",
    "plt.vlines([np.percentile(sample_rewards,50)],[0],[100],label=\"50'th percentile\",color='green')\n",
    "plt.vlines([np.percentile(sample_rewards,90)],[0],[100],label=\"90'th percentile\",color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossentropy method steps (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
    "    \n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "    \n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "    \n",
    "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
    "    \"\"\"\n",
    "    \n",
    "    reward_threshold = <Compute minimum reward for elite sessions. Hint: use np.percentile>\n",
    "    \n",
    "    \n",
    "    elite_states  = <your code here>\n",
    "    elite_actions = <your code here>\n",
    "    \n",
    "    return elite_states,elite_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states_batch = [\n",
    "    [1,2,3],   #game1\n",
    "    [4,2,0,2], #game2\n",
    "    [3,1]      #game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0,2,4],   #game1\n",
    "    [3,2,0,1], #game2\n",
    "    [3,3]      #game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,         #game1\n",
    "    4,         #game2\n",
    "    5,         #game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(states_batch,actions_batch,rewards_batch,percentile=0)\n",
    "test_result_40 = select_elites(states_batch,actions_batch,rewards_batch,percentile=30)\n",
    "test_result_90 = select_elites(states_batch,actions_batch,rewards_batch,percentile=90)\n",
    "test_result_100 = select_elites(states_batch,actions_batch,rewards_batch,percentile=100)\n",
    "\n",
    "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
    "   and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n",
    "        \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
    "        np.all(test_result_40[1] ==[3, 2, 0, 1, 3, 3]),\\\n",
    "        \"For percentile 30 you should only select states/actions from two first\"\n",
    "assert np.all(test_result_90[0] == [3,1]) and \\\n",
    "        np.all(test_result_90[1] == [3,3]),\\\n",
    "        \"For percentile 90 you should only select states/actions from one game\"\n",
    "assert np.all(test_result_100[0] == [3,1]) and\\\n",
    "       np.all(test_result_100[1] == [3,3]),\\\n",
    "        \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_policy(elite_states,elite_actions):\n",
    "    \"\"\"\n",
    "    Given old policy and a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "    \n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "    \n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "    \n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    new_policy = np.zeros([n_states,n_actions])\n",
    "    \n",
    "    <Your code here: update probabilities for actions given elite states & actions>\n",
    "    #Don't forget to set 1/n_actions for all actions in unvisited states.\n",
    "    \n",
    "    \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "elite_states, elite_actions = ([1, 2, 3, 4, 2, 0, 2, 3, 1], [0, 2, 4, 3, 2, 0, 1, 3, 3])\n",
    "\n",
    "\n",
    "new_policy = update_policy(elite_states,elite_actions)\n",
    "\n",
    "assert np.isfinite(new_policy).all(), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
    "assert np.all(new_policy>=0), \"Your new policy can't have negative action probabilities\"\n",
    "assert np.allclose(new_policy.sum(axis=-1),1), \"Your new policy should be a valid probability distribution over actions\"\n",
    "reference_answer = np.array([\n",
    "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [ 0.5       ,  0.        ,  0.        ,  0.5       ,  0.        ],\n",
    "       [ 0.        ,  0.33333333,  0.66666667,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.5       ,  0.5       ]])\n",
    "assert np.allclose(new_policy[:4,:5],reference_answer)\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(rewards_batch,log, reward_range=[-990,+10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch,percentile)\n",
    "    log.append([mean_reward,threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward,threshold))\n",
    "    plt.figure(figsize=[8,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(list(zip(*log))[0],label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1],label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(rewards_batch,range=reward_range);\n",
    "    plt.vlines([np.percentile(rewards_batch,percentile)],[0],[100],label=\"percentile\",color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop\n",
    "\n",
    "We can now combine the functions we wrote to train the agent with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_sessions = 100\n",
    "percentile = 70\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    #generate new sessions\n",
    "    sessions = [<generate a list of n_sessions new sessions by calling generate_session several times>]\n",
    "\n",
    "    states_batch,actions_batch,rewards_batch = map(np.array,zip(*sessions))\n",
    "\n",
    "    elite_states, elite_actions = <select elite states/actions for training>\n",
    "    \n",
    "    <fit agent to predict elite_actions(y) from elite_states(X)>\n",
    "\n",
    "    show_progress(rewards_batch,log,reward_range=[0,np.max(rewards_batch)])\n",
    "    \n",
    "    if np.mean(rewards_batch)> 190:\n",
    "        print(\"You Win! You may stop training now via KeyboardInterrupt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
